{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.transformer_prediction_interface.base import DoPFNRegressor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed model to be compatible with CPU, this is needed for the current version of PyTorch, see issue: https://github.com/pytorch/pytorch/issues/97128. The model will be slower if reused on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 1/1 [00:00<00:00,  2.02batch/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(ds_name=\"sales_cate\")\n",
    "dopfn = DoPFNRegressor()\n",
    "\n",
    "train_ds, test_ds = dataset.generate_valid_split(n_splits=2)\n",
    "\n",
    "dopfn.fit(train_ds.x_obs, train_ds.y_obs)\n",
    "\n",
    "x_int = deepcopy(test_ds.x_int)\n",
    "\n",
    "y_pred = dopfn.predict_full(x_int)[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3136)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((y_pred - test_ds.y_int.numpy()) / (test_ds.y_int.max() - test_ds.y_int.min())) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACIC 2016 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ACIC 2016 challenge dataset\n",
    "#\n",
    "# Sources:\n",
    "# [1] Dorie, Vincent, et al. \"Automated versus do-it-yourself methods for causal inference: Lessons learned\n",
    "# from a data analysis competition.\" (2019): 43-68.\n",
    "# [2] https://github.com/BiomedSciAI/causallib/tree/master/causallib/datasets/data/acic_challenge_2016\n",
    "#\n",
    "# The challenge includes 10 different datasets.\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CATE_Dataset:  # conditional average treatment effect\n",
    "    X_train: np.ndarray\n",
    "    t_train: np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    X_test: np.ndarray\n",
    "    true_cate: np.ndarray\n",
    "\n",
    "\n",
    "class EvalDatasetCatalog(ABC):\n",
    "    \"\"\"\n",
    "    The dataset catalog is a collection of datasets used for evaluating the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_tables: int, name: str):\n",
    "        self.n_tables = n_tables\n",
    "        self.name = name\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_tables\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        raise NotImplementedError(\"This method should be implemented by the subclass\")\n",
    "\n",
    "\n",
    "X_CSV_URL = (\n",
    "    \"https://raw.githubusercontent.com/BiomedSciAI/causallib/master/causallib/datasets/data/acic_challenge_2016/x.csv\"\n",
    ")\n",
    "\n",
    "ZY_CSV_URL = (\n",
    "    lambda i: f\"https://raw.githubusercontent.com/BiomedSciAI/causallib/master/causallib/datasets/data/acic_challenge_2016/zymu_{i}.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "class ACIC2016Dataset(EvalDatasetCatalog):\n",
    "    def __init__(self, test_ratio: float = 0.1, seed: int = 42, n_tables: int = 10):\n",
    "        super().__init__(n_tables, name=\"ACIC2016\")\n",
    "        self.test_ratio = test_ratio\n",
    "        self.x_data = pd.read_csv(X_CSV_URL)\n",
    "        self.rngs = [np.random.default_rng(seed + i) for i in range(n_tables)]\n",
    "        self.datasets = [self._get_data(i) for i in range(n_tables)]\n",
    "\n",
    "    def _get_data(self, idx: int) -> CATE_Dataset:\n",
    "        \"\"\"Loads and processes a single dataset split.\"\"\"\n",
    "        # Download file URLs\n",
    "        simulation_url = ZY_CSV_URL(idx + 1)\n",
    "\n",
    "        sim_data = pd.read_csv(simulation_url)\n",
    "\n",
    "        # Define column names for x.csv and simulation data\n",
    "        self.x_data.columns = [f\"x_{i+1}\" for i in range(self.x_data.shape[1])]\n",
    "        sim_data.columns = [\"z\", \"y0\", \"y1\", \"mu0\", \"mu1\"]\n",
    "\n",
    "        # Handle categorical variables\n",
    "        categorical_columns = [\"x_2\", \"x_21\", \"x_24\"]\n",
    "        numerical_columns = [f\"x_{i+1}\" for i in range(self.x_data.shape[1]) if f\"x_{i+1}\" not in categorical_columns]\n",
    "        self.x_data[\"x_2_numeric\"] = self.x_data[\"x_2\"].astype(\"category\").cat.codes\n",
    "        self.x_data[\"x_21_numeric\"] = self.x_data[\"x_21\"].astype(\"category\").cat.codes\n",
    "        self.x_data[\"x_24_numeric\"] = self.x_data[\"x_24\"].astype(\"category\").cat.codes\n",
    "        numerical_columns = numerical_columns + [\"x_2_numeric\", \"x_21_numeric\", \"x_24_numeric\"]\n",
    "        self.x_data = self.x_data.loc[:, numerical_columns]\n",
    "\n",
    "        # Convert to tensors\n",
    "        covariates = self.x_data.values.astype(np.float32)  # Covariates with encoded categorical variables\n",
    "        treatments = sim_data[\"z\"].values.astype(np.float32)  # Treatment\n",
    "\n",
    "        y1 = sim_data[\"y1\"].values.astype(np.float32)  # Potential outcomes under treatment\n",
    "        y0 = sim_data[\"y0\"].values.astype(np.float32)  # Potential outcomes under control\n",
    "        outcomes = np.where(treatments == 1, y1, y0)\n",
    "\n",
    "        mu0 = sim_data[\"mu0\"].values.astype(np.float32)\n",
    "        mu1 = sim_data[\"mu1\"].values.astype(np.float32)\n",
    "        cate = mu1 - mu0\n",
    "\n",
    "        # Split the dataset into train and test sets\n",
    "        indices = self.rngs[idx].permutation(covariates.shape[0])\n",
    "        split_idx = int(len(indices) * (1 - self.test_ratio))\n",
    "        train_indices = indices[:split_idx]\n",
    "        test_indices = indices[split_idx:]\n",
    "        cate_dataset = CATE_Dataset(\n",
    "            X_train=covariates[train_indices],\n",
    "            t_train=treatments[train_indices],\n",
    "            y_train=outcomes[train_indices],\n",
    "            X_test=covariates[test_indices],\n",
    "            true_cate=cate[test_indices],\n",
    "        )\n",
    "\n",
    "        return cate_dataset\n",
    "\n",
    "    def __getitem__(self, index) -> CATE_Dataset:\n",
    "        return self.datasets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.95s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.17s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.12s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.96s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:06<00:00,  6.05s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:10<00:00, 10.27s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:08<00:00,  8.20s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:07<00:00,  7.88s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.08s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.92s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.51s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.43s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.87s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.41s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.33s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.84s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:05<00:00,  5.16s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.78s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.73s/batch]\n",
      "Running inference: 100%|██████████| 1/1 [00:04<00:00,  4.69s/batch]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "dataset = ACIC2016Dataset()\n",
    "\n",
    "pehes = []\n",
    "for i in range(len(dataset)):\n",
    "    cate_dset: CATE_Dataset = dataset[i]\n",
    "    X_train = cate_dset.X_train\n",
    "    t_train = cate_dset.t_train\n",
    "    X_t_train = np.concatenate(\n",
    "        [t_train[:, None], X_train],\n",
    "        axis=1,\n",
    "    )\n",
    "    dopfn = DoPFNRegressor()\n",
    "    dopfn.fit(X_t_train, cate_dset.y_train)\n",
    "\n",
    "    x_1, x_0 = deepcopy(cate_dset.X_test), deepcopy(cate_dset.X_test)\n",
    "    X_test_0 = np.concatenate(\n",
    "        [\n",
    "            np.zeros((x_0.shape[0], 1)),\n",
    "            x_0,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    X_test_1 = np.concatenate(\n",
    "        [\n",
    "            np.ones((x_1.shape[0], 1)),\n",
    "            x_1,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    y_test_0 = dopfn.predict(torch.from_numpy(X_test_0))\n",
    "    y_test_1 = dopfn.predict(torch.from_numpy(X_test_1))\n",
    "    cate_pred = y_test_1 - y_test_0\n",
    "    pehe = np.sqrt(np.mean((cate_pred - cate_dset.true_cate) ** 2))\n",
    "    pehes.append(pehe)\n",
    "\n",
    "avg_pehe = sum(pehes) / len(pehes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PEHE over 10 datasets: 4.8290\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average PEHE over {len(dataset)} datasets: {avg_pehe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
